{"cells":[{"cell_type":"markdown","metadata":{"id":"6SixxvjLagCn"},"source":["# **Bert Implementation for Sentiment Analysis**"]},{"cell_type":"markdown","metadata":{"id":"_D3Z1rY25T_F"},"source":["BERT is currently being used at Google to optimize the interpretation of user search queries. BERT excels at several functions that make this possible, including:\n","\n","Sequence-to-sequence based language generation tasks such as:\n","- Question answering\n","- Abstract summarization\n","- Sentence prediction\n","- Conversational response generation"]},{"cell_type":"markdown","metadata":{"id":"_fTDo0GBA9rS"},"source":["# **Table Of Contents**<a name=\"Top\"></a>\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","  1. [About the Dataset](#AboutDataset)\n","  2. [Data Import](#Data-Pre)\n","  3. [Creating Training and Validation Set](#Training-Set)\n","  4. [Select Bert and Pre processing module](#Bert)\n","  5. [Passing Data to Preprocssing Module & Bert](#passingdata)\n","  6. [Training and Evaluations](#evaluations)\n","  7. [Saving and Re-Loading the Model](#Loading)\n","  8. [Predictions](#Predictions)\n","  9. [Summary](#Summary)\n","  10. [References](#References)"]},{"cell_type":"markdown","metadata":{"id":"CpgkoKpNA9oK"},"source":["# **1: About the Dataset** <a name=\"AboutDataset\"></a>\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"9kXf7TR85ij2"},"source":["This notebook trains a sentiment analysis model to classify movie reviews as positive or negative, based on the text of the review.\n","\n","The dataset contatins over 50,000 movie reviews collected from IMDB movie database"]},{"cell_type":"markdown","metadata":{"id":"lPYH_RKND7_X"},"source":["# **2: Data Import** <a name=\"Data-Pre\"></a>"]},{"cell_type":"markdown","metadata":{"id":"P3uHr9Fy6c6I"},"source":["For processing the textual information, following libraries needs to be installed\n","\n","- tensorflow\n","- medels -- for accessing Bert"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18373,"status":"ok","timestamp":1718516695898,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"z0HJF9Fpiv2C","outputId":"260a3224-c807-4f42-cb69-adb552857004"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.15.0\n","Uninstalling tensorflow-2.15.0:\n","  Successfully uninstalled tensorflow-2.15.0\n","Found existing installation: tensorflow-hub 0.16.1\n","Uninstalling tensorflow-hub-0.16.1:\n","  Successfully uninstalled tensorflow-hub-0.16.1\n","\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip uninstall -y tensorflow tensorflow-hub tensorflow-text\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44981,"status":"ok","timestamp":1718516740874,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"z1UZg6_hjJRA","outputId":"80b8c1e8-89a0-46bd-9b07-fa07e6bdce1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.15\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-hub==0.13.0\n","  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-text==2.15\n","  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\n","Installing collected packages: tensorflow-hub, tensorflow, tensorflow-text\n","Successfully installed tensorflow-2.15.0 tensorflow-hub-0.13.0 tensorflow-text-2.15.0\n"]}],"source":["!pip install tensorflow==2.15 tensorflow-hub==0.13.0 tensorflow-text==2.15"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63752,"status":"ok","timestamp":1718516804618,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"ql8oLjRyajtX","outputId":"43cc0ee7-a88b-4681-d64b-01d544a14bd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official"]},{"cell_type":"code","source":["!pip uninstall tensorflow --y\n","\n","!pip install tensorflow==2.15.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u51r5FTh7RX-","executionInfo":{"status":"ok","timestamp":1718516845495,"user_tz":-420,"elapsed":40896,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}},"outputId":"0272d934-ef1b-47f3-fcbd-4f33b4a04586"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.16.1\n","Uninstalling tensorflow-2.16.1:\n","  Successfully uninstalled tensorflow-2.16.1\n","Collecting tensorflow==2.15.0\n","  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Installing collected packages: ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.3.2\n","    Uninstalling ml-dtypes-0.3.2:\n","      Successfully uninstalled ml-dtypes-0.3.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.3.3\n","    Uninstalling keras-3.3.3:\n","      Successfully uninstalled keras-3.3.3\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.16.2\n","    Uninstalling tensorboard-2.16.2:\n","      Successfully uninstalled tensorboard-2.16.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\n","tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\n","tf-models-official 2.16.0 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0\n"]}]},{"cell_type":"code","source":["!pip freeze > requirements.txt"],"metadata":{"id":"wKwQOUa6ChIV","executionInfo":{"status":"ok","timestamp":1718516846335,"user_tz":-420,"elapsed":854,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"CRoOkhFranfK","colab":{"base_uri":"https://localhost:8080/","height":523},"executionInfo":{"status":"error","timestamp":1718516852754,"user_tz":-420,"elapsed":6423,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}},"outputId":"b2356f2a-9283-4dad-e4f1-2abe605e16b0"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"/usr/local/lib/python3.10/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2023080216raw_log_internal21internal_log_functionB5cxx11E","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-805c28e28fd5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m  \u001b[0;31m# to create AdamW optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpybinds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_registrar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.10/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2023080216raw_log_internal21internal_log_functionB5cxx11E","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","import matplotlib.pyplot as plt\n","\n","tf.get_logger().setLevel('ERROR')"]},{"cell_type":"markdown","metadata":{"id":"604tKDvX8LQJ"},"source":["Now we download the dataset using the keras utility. We then access the training dataset and remove the unwanted files."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852756,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"d6eSdHIWasg_"},"outputs":[],"source":["url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","\n","# keras uitlity to download a file\n","dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url, untar=True, cache_dir='.', cache_subdir='')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr_mI_FPbSCu","executionInfo":{"status":"aborted","timestamp":1718516852757,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["dataset_dir = '/content/aclImdb'\n","\n","train_dir = os.path.join(dataset_dir,'train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtJ9r8-dpxCa","executionInfo":{"status":"aborted","timestamp":1718516852757,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["# remove unused folders to make it easier to load the data\n","remove_dir = os.path.join(train_dir, 'unsup')\n","shutil.rmtree(remove_dir)"]},{"cell_type":"markdown","metadata":{"id":"b7PL3M53D7O4"},"source":["# **3: Creating Training and Validation Set** <a name=\"Training-Set\"></a>"]},{"cell_type":"markdown","metadata":{"id":"eMxCSmC68fTV"},"source":["Now we define the training, validation and test sets. Now there are a caveats before you use the keras preprocessing _text_data_from_directory_ method to define the training, validation and test sets.\n","\n","**caveat** -  the reviews should be under the target folder. i.e. if there is a positive review it must be under _pos_ folder similiary negative review must be under _neg_ folder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852757,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"2YO7Ul1XdN_9"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 32\n","seed = 42\n","\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train',\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='training',\n","    seed=seed)\n","\n","class_names = raw_train_ds.class_names\n","train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/train',\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=seed)\n","\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    'aclImdb/test',\n","    batch_size=batch_size)\n","\n","test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"8hDPm-9FAoFv"},"source":["acessing few samples along with its target values"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852757,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"XL3Kd7wHpUMC"},"outputs":[],"source":["for text_batch, label_batch in train_ds.take(1):\n","  for i in range(3):\n","    print(f'Review: {text_batch.numpy()[i]}')\n","    label = label_batch.numpy()[i]\n","    print(f'Label : {label} ({class_names[label]})')"]},{"cell_type":"markdown","metadata":{"id":"mYWXEMv0D7HJ"},"source":["# **4: Select Bert and Pre processing module** <a name=\"Bert\"></a>"]},{"cell_type":"markdown","metadata":{"id":"JdDK3GYuAvyC"},"source":["Out of the different variations avaliable for Bert, you can select the required configuration from the dropdown.\n","\n","As we have limited computing avaliable it is advisible to use the model which is processing lowest amount of paramerter hence I use model ending with A-2.\n","\n","A preprocessing model is also required to be selected based on the Bert model selected. The below utility helps in auto selecting the right preprocessing model. The importance of pre processing model is explained in upcoming cells"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852758,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"BbBNO9a4qDfV"},"outputs":[],"source":["#@title Choose a BERT model to fine-tune\n","\n","bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"]},{"cell_type":"markdown","metadata":{"id":"JuAYaugmD7Ac"},"source":["# **5: Passing Data to Preprocssing Module & Bert** <a name=\"passingdata\"></a>"]},{"cell_type":"markdown","metadata":{"id":"CDxjpOGSqaJz"},"source":["## The preprocessing model"]},{"cell_type":"markdown","metadata":{"id":"eVdPGmoEp3IP"},"source":["Below we pass a sample text to the preprocessing model. This model accepts 128 length of inputs hence the preprocessing is done upto 128 words. The preprocessing model converts the text into 3 keys -\n","\n","- input_mask - here each word is displayed as 1 but you will notice there are 2 addtional 1's. for instance for hello world we have four 1's instead of 2 which is one for each word. The way bert works is it put one special token in the begining and one seprator token in the end. Hence we see 2 additinal token values for every input sentence.\n","- input_type_ids - This is usually zero\n","- input_work_ids - this represt the token values for each word. not the first word i.e. the special token will always be 101 and seprator token in the end would always be 102.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vI2y8qI_qRIC","executionInfo":{"status":"aborted","timestamp":1718516852758,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852758,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"U26MIhL1qc-w"},"outputs":[],"source":["text_test = ['this is such an amazing movie!. I hate the movie', 'hello world']\n","text_preprocessed = bert_preprocess_model(text_test)\n","\n","print(f'Keys       : {list(text_preprocessed.keys())}')\n","print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n","print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n","print(f'Input Mask : {text_preprocessed[\"input_mask\"]}')\n","print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"]}')"]},{"cell_type":"markdown","metadata":{"id":"JU_uhQFiqi3B"},"source":["Using the BERT model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POsHdh-Cqet_","executionInfo":{"status":"aborted","timestamp":1718516852759,"user_tz":-420,"elapsed":25,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["bert_model = hub.KerasLayer(tfhub_handle_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852759,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"b95MpgcHqk7q"},"outputs":[],"source":["bert_results = bert_model(text_preprocessed)\n","\n","print(f'Loaded BERT: {tfhub_handle_encoder}')\n","print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n","print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n","print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n","print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"]},{"cell_type":"markdown","metadata":{"id":"tbZow1B_vw8U"},"source":["Bert output is passed to neural network and the output is predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hyrUL7ZGqm4e","executionInfo":{"status":"aborted","timestamp":1718516852759,"user_tz":-420,"elapsed":25,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["def build_classifier_model():\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n","  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852759,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"_3lINOVnqrw7"},"outputs":[],"source":["classifier_model = build_classifier_model()\n","bert_raw_result = classifier_model(tf.constant(text_test))\n","print(tf.sigmoid(bert_raw_result))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852759,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"ZfAUD4SJqrvE"},"outputs":[],"source":["tf.keras.utils.plot_model(classifier_model)"]},{"cell_type":"markdown","metadata":{"id":"9GxH3QilwCiP"},"source":["We calculate the loss using binary cross entropy for 2 class classification model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-QZ5rvJqrpw","executionInfo":{"status":"aborted","timestamp":1718516852759,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = tf.metrics.BinaryAccuracy()"]},{"cell_type":"markdown","metadata":{"id":"n9XGTeO9D64j"},"source":["# **6: Training and Evaluations** <a name=\"evaluations\"></a>"]},{"cell_type":"markdown","metadata":{"id":"1HHjaz5vwNIe"},"source":["We train the model for 3 epochs and define some hyperparameters. Lets not go to in details of these paramters and keep this tutorial simple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjswLufXqrnD","executionInfo":{"status":"aborted","timestamp":1718516852759,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["epochs = 10\n","steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","init_lr = 3e-5\n","optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P11j4Hn8qzgR","executionInfo":{"status":"aborted","timestamp":1718516852760,"user_tz":-420,"elapsed":25,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852760,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"7nzb2vowq1jH"},"outputs":[],"source":["print(f'Training model with {tfhub_handle_encoder}')\n","history = classifier_model.fit(x=train_ds,\n","                               validation_data=val_ds,\n","                               epochs=epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1718516852760,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"3ctRDCWSq3e-"},"outputs":[],"source":["loss, accuracy = classifier_model.evaluate(test_ds)\n","\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiythcODf0xo","executionInfo":{"status":"aborted","timestamp":1718516852760,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["history_dict = history.history\n","print(history_dict.keys())\n","\n","acc = history_dict['binary_accuracy']\n","val_acc = history_dict['val_binary_accuracy']\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","fig = plt.figure(figsize=(10, 6))\n","fig.tight_layout()\n","\n","plt.subplot(2, 1, 1)\n","# r is for \"solid red line\"\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","# plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(epochs, acc, 'r', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')"]},{"cell_type":"markdown","metadata":{"id":"baw3KsrxrATB"},"source":["Train and validation loss seems to be converging well, hence our model not seems to be overfitting which is great news\n"]},{"cell_type":"markdown","metadata":{"id":"ez4PXOzKD6xd"},"source":["# **7: Saving and Re-Loading the Model** <a name=\"Loading\"></a>"]},{"cell_type":"markdown","metadata":{"id":"n798PmsAxh86"},"source":["We now learn to save and reuse the saved model that helps us in saving up all the training time. To save the model we use ```.save``` method and to reload the model we use ```.saved_model.load``` method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tXjuTo2q8zr","executionInfo":{"status":"aborted","timestamp":1718516852760,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["dataset_name = 'imdb'\n","save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n","saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n","\n","classifier_model.save(saved_model_path, include_optimizer=False, options=save_options)"]},{"cell_type":"code","source":["!zip -r model.zip imdb_bert/"],"metadata":{"id":"4hFy0DqpAAB2","executionInfo":{"status":"aborted","timestamp":1718516852760,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852761,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"K9r3gj23BIVG"},"outputs":[],"source":["!pip install tensorflow onnx onnx-tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31bGvWZArCgH","executionInfo":{"status":"aborted","timestamp":1718516852761,"user_tz":-420,"elapsed":25,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["reloaded_model = tf.saved_model.load(saved_model_path)"]},{"cell_type":"markdown","metadata":{"id":"sAeDgRZSyAvZ"},"source":["You see below both the saved and reloaded model have the same prediction which confirm the saved model is working as expected."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1718516852761,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"},"user_tz":-420},"id":"VBWzH6exlCPS"},"outputs":[],"source":["def print_my_examples(inputs, results):\n","  result_for_printing = \\\n","    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n","                         for i in range(len(inputs))]\n","  print(*result_for_printing, sep='\\n')\n","  print()\n","\n","\n","examples = [\n","    'this is such an amazing movie!',  # this is the same sentence tried earlier\n","    'The movie was great!',\n","    'The movie was meh.',\n","    'The movie was okish.',\n","    'The movie was terrible...'\n","]\n","\n","reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n","original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n","\n","print('Results from the saved model:')\n","print_my_examples(examples, reloaded_results)\n","print('Results from the model in memory:')\n","print_my_examples(examples, original_results)"]},{"cell_type":"markdown","metadata":{"id":"zOkXnXIzD6p8"},"source":["# **8: Predictions** <a name=\"Predictions\"></a>"]},{"cell_type":"markdown","metadata":{"id":"vCa0hKKoyVS5"},"source":["We check our prediciton on below samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Bz3OZM1rGav","executionInfo":{"status":"aborted","timestamp":1718516852761,"user_tz":-420,"elapsed":25,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["serving_results = reloaded_model \\\n","            .signatures['serving_default'](tf.constant(examples))\n","\n","serving_results = tf.sigmoid(serving_results['classifier'])\n","\n","print_my_examples(examples, serving_results)"]},{"cell_type":"markdown","metadata":{"id":"8pkQgn1UGRvH"},"source":["# **9: Summary** <a name=\"Summary\"></a>"]},{"cell_type":"markdown","metadata":{"id":"QhufCsbdylv0"},"source":["We performed the sentiment classification using the Bert models by followning steps -\n","- Imported the dataset to our enviroment.\n","- we used keras utility fucntion ```tf.keras.preprocessing.text_dataset_from_directory``` to create training and validation sets\n","- Now we select the bert model and corresponding pre processing model.\n","- We parsed the input data to the pre processing model and understood the various key generated.\n","- We then passed the output of bert to a neural network to make the predicitons.\n","- We now trained the model and defined the hyperparamter to default values.\n","- We also learned saving and reloading the same model\n","- Finally we make prediciton using the trained model."]},{"cell_type":"markdown","metadata":{"id":"iQBt5g_TD6N5"},"source":["# **10: Reference** <a name=\"References\"></a>"]},{"cell_type":"markdown","metadata":{"id":"jsmf8jkE0It4"},"source":["- https://www.tensorflow.org/text/tutorials/classify_text_with_bert"]},{"cell_type":"markdown","metadata":{"id":"DUbbbpLm4Wjk"},"source":["# **10: Download to your Local** <a name=\"Download to local\"></a>"]},{"cell_type":"markdown","metadata":{"id":"iE5AcMvK4rO8"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUx4BPiQ4Zt4","executionInfo":{"status":"aborted","timestamp":1718516852761,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["import shutil\n","from google.colab import files\n","\n","# Path to the folder you want to compress and download\n","folder_path = '/content/aclImdb'\n","\n","# Compress the folder\n","shutil.make_archive(folder_path, 'zip', folder_path)\n","\n","# Path to the compressed file\n","zip_path = folder_path + '.zip'\n","\n","# Download the compressed file\n","files.download(zip_path)\n"]},{"cell_type":"markdown","metadata":{"id":"DsPnSrNp4tn-"},"source":["## Model Bert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9Qb7Ibh4vZH","executionInfo":{"status":"aborted","timestamp":1718516852761,"user_tz":-420,"elapsed":24,"user":{"displayName":"Wan Aufa Azis M002D4KY1554","userId":"11278704800746881856"}}},"outputs":[],"source":["import shutil\n","from google.colab import files\n","\n","# Path to the folder you want to compress and download\n","folder_path = '/content/imdb_bert'\n","\n","# Compress the folder\n","shutil.make_archive(folder_path, 'zip', folder_path)\n","\n","# Path to the compressed file\n","zip_path = folder_path + '.zip'\n","\n","# Download the compressed file\n","files.download(zip_path)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}